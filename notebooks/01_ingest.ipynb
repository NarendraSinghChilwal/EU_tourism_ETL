{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf43cde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 — Imports & SparkSession Setup\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Option A: fetch creds from environment\n",
    "ACCESS_KEY = os.environ[\"AWS_ACCESS_KEY_ID\"]\n",
    "SECRET_KEY = os.environ[\"AWS_SECRET_ACCESS_KEY\"]\n",
    "\n",
    "# Option B (uncomment to use Airflow hooks instead):\n",
    "# from airflow.providers.amazon.aws.hooks.s3 import S3Hook\n",
    "# s3 = S3Hook(aws_conn_id=\"aws_default\")\n",
    "# ACCESS_KEY = s3.aws_access_key_id\n",
    "# SECRET_KEY = s3.aws_secret_access_key\n",
    "\n",
    "S3_BUCKET = \"s3a://s3-mycollege\"\n",
    "\n",
    "# Database config from environment (or override defaults)\n",
    "DB_CONFIG = {\n",
    "    \"host\":     os.environ.get(\"PG_HOST\",    \"localhost\"),\n",
    "    \"port\":     os.environ.get(\"PG_PORT\",    \"5432\"),\n",
    "    \"user\":     os.environ.get(\"PG_USER\",    \"postgres\"),\n",
    "    \"password\": os.environ[\"PG_PASSWORD\"],\n",
    "    \"db\":       os.environ.get(\"PG_DATABASE\",\"tourism\"),\n",
    "    \"driver\":   \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "      .appName(\"01_ingest\")\n",
    "      .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "      .config(\"spark.hadoop.fs.s3a.access.key\", ACCESS_KEY)\n",
    "      .config(\"spark.hadoop.fs.s3a.secret.key\", SECRET_KEY)\n",
    "      .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.apache.hadoop:hadoop-aws:3.3.2,\"\n",
    "        \"com.amazonaws:aws-java-sdk-bundle:1.11.1026,\"\n",
    "        \"org.postgresql:postgresql:42.6.0\"\n",
    "      )\n",
    "      .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "hadoop_conf.set(\"fs.s3a.access.key\", ACCESS_KEY)\n",
    "hadoop_conf.set(\"fs.s3a.secret.key\", SECRET_KEY)\n",
    "\n",
    "\n",
    "# Cell 2 — Utility Functions\n",
    "def list_s3_csv(folder: str):\n",
    "    \"\"\"Return list of .csv paths under S3 folder.\"\"\"\n",
    "    Path = spark._jvm.org.apache.hadoop.fs.Path\n",
    "    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(\n",
    "        spark._jvm.java.net.URI(S3_BUCKET),\n",
    "        hadoop_conf\n",
    "    )\n",
    "    path = Path(f\"{S3_BUCKET}/{folder}/\")\n",
    "    files = fs.listStatus(path)\n",
    "    return [f.getPath().toString() for f in files if f.getPath().toString().endswith(\".csv\")]\n",
    "\n",
    "def prep_df(path: str, log_transform=False, winsor=(0.01,0.99)):\n",
    "    \"\"\"\n",
    "    Read CSV, filter negatives, median-impute OBS_VALUE, optional log + winsor.\n",
    "    Returns Spark DataFrame.\n",
    "    \"\"\"\n",
    "    from pyspark.sql.functions import log1p, when\n",
    "\n",
    "    df = (\n",
    "        spark.read.option(\"header\",\"true\")\n",
    "                   .option(\"inferSchema\",\"true\")\n",
    "                   .csv(path)\n",
    "                   .filter(col(\"OBS_VALUE\") >= 0)\n",
    "    )\n",
    "\n",
    "    # Median impute\n",
    "    med = df.approxQuantile(\"OBS_VALUE\",[0.5],0.001)[0]\n",
    "    df = df.fillna({\"OBS_VALUE\": med})\n",
    "\n",
    "    if log_transform:\n",
    "        df = df.withColumn(\"log_OBS_VALUE\", log1p(col(\"OBS_VALUE\")))\n",
    "        lb, ub = df.approxQuantile(\"log_OBS_VALUE\", winsor, 0.001)\n",
    "        df = df.withColumn(\n",
    "            \"log_OBS_VALUE\",\n",
    "            when(col(\"log_OBS_VALUE\") < lb, lb)\n",
    "           .when(col(\"log_OBS_VALUE\") > ub, ub)\n",
    "           .otherwise(col(\"log_OBS_VALUE\"))\n",
    "        )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Cell 3 — Ingest & Clean All Raw CSVs\n",
    "# Occupancy files\n",
    "occ_files = list_s3_csv(\"occupancy\")\n",
    "for fp in occ_files:\n",
    "    name = fp.split(\"/\")[-1]\n",
    "    do_log = any(kw in name.lower() for kw in (\"arm\",\"nim\"))\n",
    "    df = prep_df(fp, log_transform=do_log)\n",
    "\n",
    "    # write cleaned occupancy to Postgres staging table\n",
    "    jdbc_url = (\n",
    "        f\"jdbc:postgresql://{DB_CONFIG['host']}:\"\n",
    "        f\"{DB_CONFIG['port']}/{DB_CONFIG['db']}\"\n",
    "    )\n",
    "    props = {\n",
    "        \"user\": DB_CONFIG[\"user\"],\n",
    "        \"password\": DB_CONFIG[\"password\"],\n",
    "        \"driver\": DB_CONFIG[\"driver\"]\n",
    "    }\n",
    "    table = f\"staging_occ_{name.replace('.csv','')}\"\n",
    "    df.write.mode(\"overwrite\").jdbc(jdbc_url, table, properties=props)\n",
    "    print(f\"✅ Wrote cleaned occupancy → {table}\")\n",
    "\n",
    "# Capacity files\n",
    "cap_files = list_s3_csv(\"capacity\")\n",
    "for fp in cap_files:\n",
    "    name = fp.split(\"/\")[-1]\n",
    "    df = prep_df(fp, log_transform=True)\n",
    "\n",
    "    # write cleaned capacity to Postgres staging table\n",
    "    jdbc_url = (\n",
    "        f\"jdbc:postgresql://{DB_CONFIG['host']}:\"\n",
    "        f\"{DB_CONFIG['port']}/{DB_CONFIG['db']}\"\n",
    "    )\n",
    "    props = {\n",
    "        \"user\": DB_CONFIG[\"user\"],\n",
    "        \"password\": DB_CONFIG[\"password\"],\n",
    "        \"driver\": DB_CONFIG[\"driver\"]\n",
    "    }\n",
    "    table = f\"staging_cap_{name.replace('.csv','')}\"\n",
    "    df.write.mode(\"overwrite\").jdbc(jdbc_url, table, properties=props)\n",
    "    print(f\"✅ Wrote cleaned capacity → {table}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa401ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
