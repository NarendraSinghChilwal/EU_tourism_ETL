{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9be86ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 — SparkSession & Read Merged Yearly Data\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg\n",
    "import os\n",
    "\n",
    "# JDBC / Airflow‑injected params\n",
    "DB_CONFIG = {\n",
    "    \"host\":     \"localhost\",\n",
    "    \"port\":     \"5432\",\n",
    "    \"user\":     \"postgres\",\n",
    "    \"password\": \"1234\",\n",
    "    \"db\":       \"tourism\",\n",
    "    \"driver\":   \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"08_cluster\")\n",
    "    .config(\"spark.jars.packages\",\"org.postgresql:postgresql:42.6.0\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "jdbc_url = f\"jdbc:postgresql://{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['db']}\"\n",
    "props = {\n",
    "    \"user\": DB_CONFIG[\"user\"],\n",
    "    \"password\": DB_CONFIG[\"password\"],\n",
    "    \"driver\": DB_CONFIG[\"driver\"]\n",
    "}\n",
    "\n",
    "# Load the merged occupancy+capacity yearly table\n",
    "df = spark.read.jdbc(jdbc_url, \"tourism_merged_yearly\", properties=props)\n",
    "\n",
    "# EU member states list\n",
    "eu_states = [\n",
    "    \"Austria\",\"Belgium\",\"Bulgaria\",\"Croatia\",\"Cyprus\",\"Czechia\",\"Denmark\",\n",
    "    \"Estonia\",\"Finland\",\"France\",\"Germany\",\"Greece\",\"Hungary\",\"Ireland\",\"Italy\",\n",
    "    \"Latvia\",\"Lithuania\",\"Luxembourg\",\"Malta\",\"Netherlands\",\"Poland\",\"Portugal\",\n",
    "    \"Romania\",\"Slovakia\",\"Slovenia\",\"Spain\",\"Sweden\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b19af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 — Build country_summary for clustering\n",
    "#  avg occupancy_rate & avg capacity per country\n",
    "country_summary = (\n",
    "    df.filter(col(\"geo\").isin(eu_states))\n",
    "      .groupBy(\"geo\")\n",
    "      .agg(\n",
    "        avg(\"occupancy_rate\").alias(\"mean_occupancy\"),\n",
    "        avg(\"log_capacity_sum\").alias(\"mean_log_capacity\")\n",
    "      )\n",
    "      .toPandas()\n",
    ")\n",
    "\n",
    "print(\"Country summary:\")\n",
    "print(country_summary.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952b1142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 — Silhouette search & KMeans clustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(country_summary[[\"mean_occupancy\",\"mean_log_capacity\"]])\n",
    "\n",
    "# find best k\n",
    "scores = []\n",
    "for k in range(2,7):\n",
    "    km = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = km.fit_predict(X_scaled)\n",
    "    scores.append((k, silhouette_score(X_scaled, labels)))\n",
    "\n",
    "best_k, best_score = max(scores, key=lambda x: x[1])\n",
    "print(f\"Silhouette scores by k: {scores}\")\n",
    "print(f\"→ Selecting k = {best_k} (score={best_score:.3f})\")\n",
    "\n",
    "# fit final model\n",
    "kmeans = KMeans(n_clusters=best_k, random_state=42)\n",
    "country_summary[\"cluster\"] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "print(\"\\nCluster assignments:\")\n",
    "print(country_summary.sort_values(\"cluster\")[[\"geo\",\"mean_occupancy\",\"mean_log_capacity\",\"cluster\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862b3920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 — Persist clusters to Postgres\n",
    "# so mapping notebook can read them\n",
    "cluster_sdf = spark.createDataFrame(country_summary[[\"geo\",\"cluster\"]])\n",
    "\n",
    "cluster_sdf.write.mode(\"overwrite\") \\\n",
    "    .jdbc(jdbc_url, \"tourism_clusters\", properties=props)\n",
    "\n",
    "print(\"Written cluster assignments to table 'tourism_clusters'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
